{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from math import floor\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import keras_video.utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import (ImageDataGenerator, img_to_array)\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPool2D, GlobalMaxPool2D, Dense, Dropout, Flatten"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def build_mobile(shape=(224, 224, 3)):\n",
    "    model = keras.applications.mobilenet.MobileNet(\n",
    "        include_top=False,\n",
    "        input_shape=shape,\n",
    "        weights='imagenet')\n",
    "\n",
    "    trainable = 4\n",
    "    for layer in model.layers[:-trainable]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[-trainable:]:\n",
    "        layer.trainable = True\n",
    "    output = GlobalMaxPool2D()\n",
    "    return keras.Sequential([model, output])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def add_top_model(num_classes):\n",
    "    mobile = build_mobile()\n",
    "    # then create our final model\n",
    "    model = keras.Sequential()\n",
    "    model.add(mobile)\n",
    "    # add the convnet with (5, 224, 224, 3) shape\n",
    "    model.add(Flatten(name=\"flatten\"))\n",
    "    # here, you can also use GRU or LSTM\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 1024)              3228864   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               262400    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,524,418\n",
      "Trainable params: 1,346,178\n",
      "Non-trainable params: 2,178,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_classes = 2\n",
    "model = add_top_model(n_classes)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 722 images belonging to 2 classes.\n",
      "<keras.preprocessing.image.DirectoryIterator object at 0x000001B73EEBBB50>\n",
      "Found 52 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = './pigs_tail/train'\n",
    "validation_data_dir = './pigs_tail/validation'\n",
    "\n",
    "# Let's use some data augmentaiton\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# set our batch size (typically on most mid tier systems we'll use 16-32)\n",
    "batch_size = 16\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    save_to_dir='crop-tail',\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "model.compile(\n",
    "    optimizer,\n",
    "    'categorical_crossentropy',\n",
    "    metrics=['acc']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1076 - acc: 0.9681\n",
      "Epoch 00001: saving model to pig_tails_checkpoint\\weights.01-0.18.hdf5\n",
      "46/46 [==============================] - 20s 442ms/step - loss: 0.1076 - acc: 0.9681 - val_loss: 0.1788 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1049 - acc: 0.9681\n",
      "Epoch 00002: saving model to pig_tails_checkpoint\\weights.02-0.18.hdf5\n",
      "46/46 [==============================] - 26s 559ms/step - loss: 0.1049 - acc: 0.9681 - val_loss: 0.1767 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0877 - acc: 0.9695\n",
      "Epoch 00003: saving model to pig_tails_checkpoint\\weights.03-0.18.hdf5\n",
      "46/46 [==============================] - 25s 543ms/step - loss: 0.0877 - acc: 0.9695 - val_loss: 0.1769 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0765 - acc: 0.9723\n",
      "Epoch 00004: saving model to pig_tails_checkpoint\\weights.04-0.18.hdf5\n",
      "46/46 [==============================] - 23s 511ms/step - loss: 0.0765 - acc: 0.9723 - val_loss: 0.1791 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1103 - acc: 0.9612\n",
      "Epoch 00005: saving model to pig_tails_checkpoint\\weights.05-0.18.hdf5\n",
      "46/46 [==============================] - 21s 449ms/step - loss: 0.1103 - acc: 0.9612 - val_loss: 0.1759 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 6/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9708\n",
      "Epoch 00006: saving model to pig_tails_checkpoint\\weights.06-0.18.hdf5\n",
      "46/46 [==============================] - 26s 549ms/step - loss: 0.0917 - acc: 0.9709 - val_loss: 0.1766 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0939 - acc: 0.9695\n",
      "Epoch 00007: saving model to pig_tails_checkpoint\\weights.07-0.18.hdf5\n",
      "46/46 [==============================] - 21s 454ms/step - loss: 0.0939 - acc: 0.9695 - val_loss: 0.1794 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 8/100\n",
      "45/46 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9750\n",
      "Epoch 00008: saving model to pig_tails_checkpoint\\weights.08-0.18.hdf5\n",
      "46/46 [==============================] - 23s 492ms/step - loss: 0.0748 - acc: 0.9751 - val_loss: 0.1819 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0974 - acc: 0.9723\n",
      "Epoch 00009: saving model to pig_tails_checkpoint\\weights.09-0.18.hdf5\n",
      "46/46 [==============================] - 27s 598ms/step - loss: 0.0974 - acc: 0.9723 - val_loss: 0.1827 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.1112 - acc: 0.9571\n",
      "Epoch 00010: saving model to pig_tails_checkpoint\\weights.10-0.18.hdf5\n",
      "46/46 [==============================] - 22s 469ms/step - loss: 0.1112 - acc: 0.9571 - val_loss: 0.1833 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0952 - acc: 0.9626\n",
      "Epoch 00011: saving model to pig_tails_checkpoint\\weights.11-0.18.hdf5\n",
      "46/46 [==============================] - 21s 459ms/step - loss: 0.0952 - acc: 0.9626 - val_loss: 0.1816 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0846 - acc: 0.9792\n",
      "Epoch 00012: saving model to pig_tails_checkpoint\\weights.12-0.18.hdf5\n",
      "46/46 [==============================] - 19s 414ms/step - loss: 0.0846 - acc: 0.9792 - val_loss: 0.1811 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0972 - acc: 0.9668\n",
      "Epoch 00013: saving model to pig_tails_checkpoint\\weights.13-0.18.hdf5\n",
      "46/46 [==============================] - 22s 475ms/step - loss: 0.0972 - acc: 0.9668 - val_loss: 0.1810 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0956 - acc: 0.9751\n",
      "Epoch 00014: saving model to pig_tails_checkpoint\\weights.14-0.18.hdf5\n",
      "46/46 [==============================] - 19s 403ms/step - loss: 0.0956 - acc: 0.9751 - val_loss: 0.1806 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0922 - acc: 0.9654\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "\n",
      "Epoch 00015: saving model to pig_tails_checkpoint\\weights.15-0.18.hdf5\n",
      "46/46 [==============================] - 21s 465ms/step - loss: 0.0922 - acc: 0.9654 - val_loss: 0.1783 - val_acc: 0.9231 - lr: 1.0000e-10\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.0801 - acc: 0.9792\n",
      "Epoch 00016: saving model to pig_tails_checkpoint\\weights.16-0.18.hdf5\n",
      "46/46 [==============================] - 25s 550ms/step - loss: 0.0801 - acc: 0.9792 - val_loss: 0.1772 - val_acc: 0.9231 - lr: 1.0000e-11\n",
      "Epoch 17/100\n",
      " 1/46 [..............................] - ETA: 25s - loss: 0.0374 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "EPOCHS = 100\n",
    "# create a \"pig_tails_checkpoint\" directory before to run that\n",
    "# because ModelCheckpoint will write models inside\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(verbose=1),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'pig_tails_checkpoint/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "        verbose=1),\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    verbose=1,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def discover_classes():\n",
    "    # use sub directories names as classes\n",
    "    classes = [i.split(os.path.sep)[1] for i in glob.glob('./pigs_tail/train/*')]\n",
    "    classes.sort()\n",
    "\n",
    "    return classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def predict(model_, classes, img_path):\n",
    "    input_im = cv2.imread(img_path)\n",
    "\n",
    "    input_im = cv2.resize(input_im, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "    input_im = input_im / 255.\n",
    "    input_im = input_im.reshape(1, 224, 224, 3)\n",
    "    predictions = model_.predict(input_im)\n",
    "\n",
    "    closest_guess = None\n",
    "    index_guess = 0\n",
    "    prob_guess = 0\n",
    "\n",
    "    for pre in predictions[0]:\n",
    "        if pre > prob_guess:\n",
    "            prob_guess = pre\n",
    "            closest_guess = index_guess\n",
    "        index_guess = index_guess + 1\n",
    "\n",
    "    return classes[closest_guess]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def display_test_result(name, pred, img_path):\n",
    "    input_im = cv2.imread(img_path)\n",
    "    input_original = input_im.copy()\n",
    "    BLACK = [0, 0, 0]\n",
    "    expanded_image = cv2.copyMakeBorder(input_original, 80, 0, 0, 100, cv2.BORDER_CONSTANT, value=BLACK)\n",
    "    cv2.putText(expanded_image, pred, (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.imshow(name, expanded_image)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "files = [f for f in listdir('./test-img') if isfile(join('./test-img', f))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "cls = discover_classes()\n",
    "tail_classifier = keras.models.load_model('tail-classifier.hdf5')\n",
    "\n",
    "for file in files:\n",
    "    guess = predict(tail_classifier, cls, f'test-img/{file}')\n",
    "    display_test_result(file, guess, f'test-img/{file}')\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#get back the webcam\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}